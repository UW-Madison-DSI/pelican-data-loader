{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ab5e17",
   "metadata": {},
   "source": [
    "# Image dataset\n",
    "\n",
    "- Official Croissant example of MNIST: https://github.com/mlcommons/croissant/tree/main/datasets/1.0/huggingface-mnist\n",
    "- Dataset: https://huggingface.co/datasets/ylecun/mnist\n",
    "- For small image files, the converted the images inside a parquet, so that it is more portable, perhaps not efficient for larger datasets\n",
    "- Split example: https://github.com/mlcommons/croissant/blob/main/docs/howto/specify-splits.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78131d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc00151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "from mlcroissant import Dataset\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pelican_data_loader.config import SYSTEM_CONFIG\n",
    "from pelican_data_loader.utils import get_sha256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaaee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using a Git Repo to host data, we will use our development S3 bucket.\n",
    "# from pelican_data_loader.data import upload_to_s3\n",
    "\n",
    "# upload_to_s3(\n",
    "#     file_path=Path(\"tmp/mnist/test-00000-of-00001.parquet\"),\n",
    "#     object_name=\"mnist/test.parquet\",\n",
    "# )\n",
    "\n",
    "# upload_to_s3(\n",
    "#     file_path=Path(\"tmp/mnist/train-00000-of-00001.parquet\"),\n",
    "#     object_name=\"mnist/train.parquet\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a84fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the official metadata for the MNIST dataset in the Croissant format\n",
    "# For reference\n",
    "# import httpx\n",
    "\n",
    "# response = httpx.get(\n",
    "#     \"https://raw.githubusercontent.com/mlcommons/croissant/refs/heads/main/datasets/1.0/huggingface-mnist/metadata.json\"\n",
    "# )\n",
    "# metadata = response.json()\n",
    "# Path(\"tmp/mnist/metadata.json\").write_text(json.dumps(metadata, indent=2))\n",
    "\n",
    "# This is not optimal / usable, because\n",
    "# 1. Broken sha256, see below s3 FileObject representation\n",
    "# 2. Even with the correct sha256, mlcroissant does not download the file set correctly from s3 with RecordSet\n",
    "# 3. It also depends on a very slow way to obtain the train/test split columns dynamically from the file path, which could be useful for larger datasets organized in folder structure. Maybe explore it later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_s3 = mlc.FileObject(\n",
    "#     id=\"wisc_s3\",\n",
    "#     name=\"wisc_S3\",\n",
    "#     description=\"WISC Pelican Data Loader S3 bucket\",\n",
    "#     content_url=f\"https://{SYSTEM_CONFIG.s3_url}/\",\n",
    "#     encoding_formats=[\"https\"],  # https is the protocol used for S3\n",
    "#     sha256=\"21032ff9ec7c6aed6b2a34399af57090a4195bf099e9846a386c48a1d3260376\",  # Placeholder like in official metadata\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac5d91",
   "metadata": {},
   "source": [
    "IMPORTANT LIMITATION on S3 bucket as `FileObject`:\n",
    "\n",
    "- The official example suggested we should use `FileObject` to represent a S3 bucket\n",
    "- Currently sha256 is required, see [issue#813](https://github.com/mlcommons/croissant/issues/813)\n",
    "- But an s3 storage hash will change after something is uploaded to it, so the sha256 will then be changed, there is no way we can \n",
    "\n",
    "Example\n",
    "\n",
    "```py\n",
    "import httpx\n",
    "from pelican_data_loader.utils import get_sha256_from_bytes\n",
    "\n",
    "response = httpx.get(f\"https://{SYSTEM_CONFIG.s3_url}/\")\n",
    "get_sha256_from_bytes(response.text.encode(\"utf-8\"))\n",
    "> '21032ff9ec7c6aed6b2a34399af57090a4195bf099e9846a386c48a1d3260376'\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This represent the parquet file set that contains the MNIST dataset\n",
    "\n",
    "# BROKEN\n",
    "\n",
    "# parquet_file_set = mlc.FileSet(\n",
    "#     id=\"parquet_files\",\n",
    "#     name=\"parquet_files\",\n",
    "#     description=\"Parquet files for MNIST dataset\",\n",
    "#     encoding_formats=[mlc.EncodingFormat.PARQUET],\n",
    "#     contained_in=[dev_s3.id],\n",
    "#     # includes=[\"mnist/train.parquet\", \"mnist/test.parquet\"],\n",
    "#     includes=[\"mnist/train.parquet\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mlc.FileObject(\n",
    "    id=\"mnist_train\",\n",
    "    name=\"mnist_train\",\n",
    "    description=\"MNIST training data in Parquet format\",\n",
    "    encoding_formats=[mlc.EncodingFormat.PARQUET],\n",
    "    content_url=f\"https://{SYSTEM_CONFIG.s3_url}/mnist/train.parquet\",\n",
    "    sha256=get_sha256(Path(\"tmp/mnist/train-00000-of-00001.parquet\")),\n",
    ")\n",
    "\n",
    "test_data = mlc.FileObject(\n",
    "    id=\"mnist_test\",\n",
    "    name=\"mnist_test\",\n",
    "    description=\"MNIST test data in Parquet format\",\n",
    "    encoding_formats=[mlc.EncodingFormat.PARQUET],\n",
    "    content_url=f\"https://{SYSTEM_CONFIG.s3_url}/mnist/test.parquet\",\n",
    "    sha256=get_sha256(Path(\"tmp/mnist/test-00000-of-00001.parquet\")),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bd1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_record_set(id: str, file_object: mlc.FileObject) -> mlc.RecordSet:\n",
    "    \"\"\"Create a RecordSet for the MNIST dataset.\"\"\"\n",
    "\n",
    "    image_field = mlc.Field(\n",
    "        id=f\"{id}/image\",\n",
    "        name=f\"{id}/image\",\n",
    "        description=\"Image column in the parquet file\",\n",
    "        data_types=[mlc.DataType.IMAGE_OBJECT],\n",
    "        source=mlc.Source(\n",
    "            file_object=file_object.id,\n",
    "            extract=mlc.Extract(column=\"image\"),\n",
    "            transforms=[mlc.Transform(json_path=\"bytes\")],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    label_field = mlc.Field(\n",
    "        id=f\"{id}/label\",\n",
    "        name=f\"{id}/label\",\n",
    "        description=\"Label column in the parquet file\",\n",
    "        data_types=[mlc.DataType.INTEGER],\n",
    "        source=mlc.Source(file_object=file_object.id, extract=mlc.Extract(column=\"label\")),\n",
    "    )\n",
    "\n",
    "    return mlc.RecordSet(\n",
    "        id=id,\n",
    "        name=id,\n",
    "        description=f\"{id.title()} record set for MNIST dataset\",\n",
    "        fields=[image_field, label_field],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682bbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_metadata = mlc.Metadata(\n",
    "    name=\"mnist\",\n",
    "    description=\"The MNIST dataset consists of 70,000 28x28 black-and-white images in 10 classes (one for each digits), with 7,000\\nimages per class. There are 60,000 training images and 10,000 test images.\\n\",\n",
    "    version=\"1.0.0\",\n",
    "    license=[\"mit\"],\n",
    "    cite_as=\"CITE_ME_PLACEHOLDER\",\n",
    "    url=\"https://huggingface.co/datasets/mnist\",\n",
    "    distribution=[train_data, test_data],\n",
    "    record_sets=[create_record_set(\"train\", train_data), create_record_set(\"test\", test_data)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae40bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_metadata.issues.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch date\n",
    "\n",
    "jsonld = mnist_metadata.to_json()\n",
    "jsonld[\"datePublished\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "Path(\"tmp/mnist/replicated_metadata.json\").write_text(json.dumps(jsonld, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26682b",
   "metadata": {},
   "source": [
    "## Loading test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official\n",
    "\n",
    "# dataset_via_official_metadata = Dataset(jsonld=\"tmp/mnist/metadata.json\")\n",
    "# official_records = dataset_via_official_metadata.records(\"default\")\n",
    "# for x in official_records:\n",
    "#     image = x[\"default/image\"]\n",
    "#     image.show()\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f75602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicated\n",
    "\n",
    "dataset_recreated = Dataset(jsonld=\"tmp/mnist/replicated_metadata.json\")\n",
    "records = dataset_recreated.records(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(records):\n",
    "    print(x)\n",
    "    x[\"test/image\"].show()\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c278a",
   "metadata": {},
   "source": [
    "### Upload metadata to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ba6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pelican_data_loader.data import upload_to_s3\n",
    "\n",
    "upload_to_s3(\n",
    "    file_path=Path(\"tmp/mnist/replicated_metadata.json\"),\n",
    "    object_name=\"metadata/mnist.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a75d5",
   "metadata": {},
   "source": [
    "### Upload to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pelican_data_loader.db import Dataset, get_session\n",
    "\n",
    "jsonld = json.loads(Path(\"tmp/mnist/replicated_metadata.json\").read_text())\n",
    "mnist = Dataset.from_jsonld(jsonld)\n",
    "mnist.croissant_jsonld_url = f\"{SYSTEM_CONFIG.s3_url}/metadata/mnist.json\"\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_session() as session:\n",
    "    session.add(mnist)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36498e",
   "metadata": {},
   "source": [
    "TODO: Cater for multiple files distributions dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pelican-data-loader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
